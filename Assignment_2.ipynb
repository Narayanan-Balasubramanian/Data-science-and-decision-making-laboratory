{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNLxnQGxxg0UdZ4D3XzpO5a",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nb20593/data-science/blob/main/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM86gV5JY7lW"
      },
      "source": [
        "Installing transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiM3J6ScYnEH",
        "outputId": "99f410ed-6c88-45da-850b-25e58db20136"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 30.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 43.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqP0M4NhZFj_"
      },
      "source": [
        "import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gmEfjM0ZNjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7502b7ee-4ff5-468e-d7f8-b1566075ae49"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "from sklearn.externals import joblib\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import transformers as ppb # pytorch transformers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFO5w5jUZVGr"
      },
      "source": [
        "datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ptGrJu6ZbC3"
      },
      "source": [
        "irony_train_label_data =  pd.read_fwf('https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/irony/train_labels.txt',header=None)\n",
        "irony_train_text_data = pd.read_fwf('https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/irony/train_text.txt',header=None)\n",
        "irony_test_label_data =pd.read_fwf ('https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/irony/test_labels.txt',header=None)\n",
        "irony_test_text_data = pd.read_fwf ('https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/irony/test_text.txt',header=None)\n",
        "irony_val_label_data = pd.read_fwf ('https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/irony/val_labels.txt',header=None)\n",
        "irony_val_text_data = pd.read_fwf ('https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/irony/val_text.txt',header=None)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhDZy3p-bcBZ"
      },
      "source": [
        "Verify the shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw6Ff687bM5E",
        "outputId": "097a4ebc-96be-49f2-abb4-2d25fa8700c4"
      },
      "source": [
        "print(irony_train_label_data.shape)\n",
        "print(irony_train_text_data.shape)\n",
        "print(irony_test_label_data.shape)\n",
        "print(irony_test_text_data.shape)\n",
        "print(irony_val_label_data.shape)\n",
        "print(irony_val_text_data.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2862, 1)\n",
            "(2862, 1)\n",
            "(784, 1)\n",
            "(784, 1)\n",
            "(955, 1)\n",
            "(955, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tLWHJ_ueCsO"
      },
      "source": [
        "Combine text and label columns\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "inZC2cbxbj-P",
        "outputId": "840e2ba1-0a58-48c2-c053-b4618199bccb"
      },
      "source": [
        "df_irony_train = irony_train_text_data.join(irony_train_label_data,rsuffix = \"_2\")\n",
        "df_irony_test = irony_test_text_data.join(irony_test_label_data,rsuffix = \"_2\")\n",
        "df_irony_val = irony_val_text_data.join(irony_val_label_data,rsuffix = \"_2\")\n",
        "df_irony_train.columns = ['text','label']\n",
        "df_irony_test.columns = ['text','label']\n",
        "df_irony_val.columns = ['text','label']\n",
        "df_irony_val.head(5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#NBA players #NY support protests of #police k...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A new year about to start|So many people came ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Obama's $1,176,120.90 in Taxpayer Funded Costs...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Can't wait to work with the dream team again t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>!!! RT @user Of all the places to get stuck in...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  #NBA players #NY support protests of #police k...      1\n",
              "1  A new year about to start|So many people came ...      0\n",
              "2  Obama's $1,176,120.90 in Taxpayer Funded Costs...      1\n",
              "3  Can't wait to work with the dream team again t...      1\n",
              "4  !!! RT @user Of all the places to get stuck in...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMZGmVfLUhBb"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4Y8ZJSreKU6"
      },
      "source": [
        "Remove contradictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAtASshWee0q"
      },
      "source": [
        " contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
        "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
        "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
        "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
        "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
        "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
        "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
        "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
        "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
        "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
        "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
        "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
        "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
        "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
        "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
        "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
        "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
        "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
        "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
        "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
        "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
        "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
        "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
        "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
        "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
        "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
        "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
        "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
        "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
        "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
        "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
        "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
        "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
        "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
        "                     \"you've\": \"you have\"}\n",
        "\n",
        "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "\n",
        "def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "  def replace(match):\n",
        "    return contractions_dict[match.group(0)]\n",
        "  return contractions_re.sub(replace, text)\n",
        "\n",
        "df_irony_train['text']=df_irony_train['text'].apply(lambda x:expand_contractions(x))\n",
        "df_irony_test['text'] = df_irony_test['text'].apply(lambda x:expand_contractions(x))\n",
        "df_irony_val['text'] = df_irony_val['text'].apply(lambda x:expand_contractions(x))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tPrUR59gr19"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGoebmzVe44k"
      },
      "source": [
        "def processTweet(tweet):\n",
        "    # Remove HTML special entities (e.g. &amp;)\n",
        "    tweet = re.sub(r'\\&\\w*;', '', tweet)\n",
        "    #Convert @username to AT_USER\n",
        "    tweet = re.sub('@[^\\s]+','',tweet)\n",
        "    # Remove tickers\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # To lowercase\n",
        "    tweet = tweet.lower()\n",
        "    # Remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*\\/\\w*', '', tweet)\n",
        "    # Remove hashtags\n",
        "    tweet = re.sub(r'#\\w*', '', tweet)\n",
        "    # Remove whitespace (including new line characters)\n",
        "    tweet = re.sub(r'\\s\\s+', ' ', tweet)\n",
        "    # Remove single space remaining at the front of the tweet.\n",
        "    tweet = tweet.lstrip(' ') \n",
        "    # Remove digits\n",
        "    tweet = re.sub(r'\\w*\\d\\w*','', tweet)\n",
        "    #Remove puntuation\n",
        "    tweet = re.sub(r'[%s]' % re.escape(string.punctuation), '', tweet)\n",
        "    #Remove extra spaces\n",
        "    tweet = re.sub(' +',' ',tweet)\n",
        "    return tweet"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTzDACh1fGDG"
      },
      "source": [
        "df_irony_train['text'] = df_irony_train['text'].apply(processTweet)\n",
        "df_irony_test['text'] = df_irony_test['text'].apply(processTweet)\n",
        "df_irony_val['text'] = df_irony_val['text'].apply(processTweet)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c85-WmJXgkZA"
      },
      "source": [
        "Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMUSMKxJf9y3"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Loading model\n",
        "nlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])\n",
        "\n",
        "df_irony_train['text']=df_irony_train['text'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))\n",
        "df_irony_test['text']=df_irony_test['text'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))\n",
        "df_irony_val['text']=df_irony_val['text'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rY-FRaThask"
      },
      "source": [
        "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC4fbk-lh05_"
      },
      "source": [
        "train_tokenized = df_irony_train['text'].apply((lambda x: tokenizer.encode(x,add_special_tokens=True,padding=True,truncation=True)))\n",
        "test_tokenized = df_irony_test['text'].apply((lambda x: tokenizer.encode(x,add_special_tokens=True,padding=True,truncation=True)))\n",
        "val_tokenized = df_irony_val['text'].apply((lambda x: tokenizer.encode(x,add_special_tokens=True,padding=True,truncation=True)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3bMqOCziRMg"
      },
      "source": [
        "max_len = 0\n",
        "for i in train_tokenized.values:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "\n",
        "train_padded = np.array([i + [0]*(40-len(i)) for i in train_tokenized.values])\n",
        "test_padded = np.array([i + [0]*(40-len(i)) for i in test_tokenized.values])\n",
        "val_padded = np.array([i + [0]*(40-len(i)) for i in val_tokenized.values])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2qkyhPOi0SK"
      },
      "source": [
        "print(np.array(test_padded).shape,np.array(train_padded).shape,np.array(val_padded).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qewoyu3NwoS9"
      },
      "source": [
        "train_attention_mask = np.where(train_padded != 0, 1, 0)\n",
        "test_attention_mask = np.where(test_padded != 0, 1, 0)\n",
        "val_attention_mask = np.where(val_padded != 0,1,0)\n",
        "test_attention_mask.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtX64duowP29"
      },
      "source": [
        "train_input_ids = torch.tensor(train_padded)  \n",
        "train_attention_mask = torch.tensor(train_attention_mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_last_hidden_states = model(train_input_ids, attention_mask=train_attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjbdV8Sjx6rQ"
      },
      "source": [
        "test_input_ids = torch.tensor(test_padded)\n",
        "test_attention_mask = torch.tensor(test_attention_mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "   test_last_hidden_states = model(test_input_ids, attention_mask=test_attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJnPeV6syOiN"
      },
      "source": [
        "val_input_ids = torch.tensor(val_padded)\n",
        "val_attention_mask = torch.tensor(val_attention_mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "   val_last_hidden_states = model(val_input_ids, attention_mask=val_attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OboWB4Wymx7"
      },
      "source": [
        "train_features = train_last_hidden_states[0][:,0,:].numpy()\n",
        "test_features = test_last_hidden_states[0][:,0,:].numpy()\n",
        "val_features = val_last_hidden_states[0][:,0,:].numpy()\n",
        "train_labels = torch.tensor(df_irony_train['label'])\n",
        "test_labels = torch.tensor(df_irony_test['label'])\n",
        "val_labels = torch.tensor(df_irony_val['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf2URINGVNeK"
      },
      "source": [
        "random_forest.fit(train_features, train_labels)\n",
        "random_forest.score(test_features, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIBNIWOty2Ra"
      },
      "source": [
        "lr_clf = LogisticRegression(max_iter=1000)\n",
        "lr_clf.fit(train_features, train_labels)\n",
        "lr_clf.score(test_features, test_labels)\n",
        "lr_clf.score(val_features, val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKeqnHEaSoot"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "svc_regressor = SVC()\n",
        "\n",
        "scores = cross_val_score(svc_regressor, train_features, train_labels)\n",
        "print(\"SVM classifier average score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lq_K9Oi8Sm6"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\n",
        "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\n",
        "grid.fit(train_features,train_labels)\n",
        "print(grid.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-mu9BuzEhdO"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix \n",
        "grid_predictions = grid.predict(test_features)\n",
        "print(confusion_matrix(test_labels,grid_predictions))\n",
        "print(classification_report(test_labels,grid_predictions))\n",
        "grid_predictions = grid.predict(val_features)\n",
        "print(classification_report(val_labels,grid_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXbNWvFpVZme"
      },
      "source": [
        "svc_regressor.fit(train_features, train_labels)\n",
        "svc_regressor.score(test_features, test_labels)\n",
        "print (\"accuracy from svm: \",svc_regressor.score(test_features, test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT2eerAaTSlJ"
      },
      "source": [
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "neural_network = MLPClassifier(hidden_layer_sizes=(2000), learning_rate='adaptive', verbose=False)\n",
        "neural_network.fit(train_features, train_labels)\n",
        "neural_network.score(test_features, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmsF9GinIyy9"
      },
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "clf = DummyClassifier()\n",
        "\n",
        "scores = cross_val_score(clf, train_features, train_labels)\n",
        "print(\"Dummy classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfIttYueSYZy"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "random_forest = RandomForestClassifier()\n",
        "scores = cross_val_score(random_forest, train_features, train_labels)\n",
        "print(\"Random forest classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW_jMp78rgYI"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    if axes is None:\n",
        "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "    axes[0].set_title(title)\n",
        "    if ylim is not None:\n",
        "        axes[0].set_ylim(*ylim)\n",
        "    axes[0].set_xlabel(\"Training examples\")\n",
        "    axes[0].set_ylabel(\"Score\")\n",
        "\n",
        "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
        "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
        "                       train_sizes=train_sizes,\n",
        "                       return_times=True)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    fit_times_mean = np.mean(fit_times, axis=1)\n",
        "    fit_times_std = np.std(fit_times, axis=1)\n",
        "\n",
        "    # Plot learning curve\n",
        "    axes[0].grid()\n",
        "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                         color=\"r\")\n",
        "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                         color=\"g\")\n",
        "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "                 label=\"Training score\")\n",
        "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "                 label=\"Cross-validation score\")\n",
        "    axes[0].legend(loc=\"best\")\n",
        "\n",
        "    # Plot n_samples vs fit_times\n",
        "    axes[1].grid()\n",
        "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
        "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
        "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
        "    axes[1].set_xlabel(\"Training examples\")\n",
        "    axes[1].set_ylabel(\"fit_times\")\n",
        "    axes[1].set_title(\"Scalability of the model\")\n",
        "\n",
        "    # Plot fit_time vs score\n",
        "    axes[2].grid()\n",
        "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
        "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
        "    axes[2].set_xlabel(\"fit_times\")\n",
        "    axes[2].set_ylabel(\"Score\")\n",
        "    axes[2].set_title(\"Performance of the model\")\n",
        "\n",
        "    return plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itf29_uQr2uk"
      },
      "source": [
        "title = r\"Learning Curves (SVM, sigmoid kernel, $\\gamma=0.001$)\"\n",
        "# SVC is more expensive so we do a lower number of CV iterations:\n",
        "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=10)\n",
        "estimator = SVC(gamma=0.001,kernel = 'sigmoid')\n",
        "plot_learning_curve(estimator, title,train_features, train_labels,cv=cv, n_jobs=4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRHe_1UQEwuO"
      },
      "source": [
        "offensive_train_label_data =  pd.read_fwf('https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/train_labels.txt',header=None)\n",
        "offensive_train_text_data = pd.read_fwf('https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/train_text.txt',header=None)\n",
        "offensive_test_label_data =pd.read_fwf ('https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/test_labels.txt',header=None)\n",
        "offensive_test_text_data = pd.read_fwf ('https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/test_text.txt',header=None)\n",
        "offensive_val_label_data = pd.read_fwf ('https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/val_labels.txt',header=None)\n",
        "offensive_val_text_data = pd.read_fwf ('https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/val_text.txt',header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUusueSUEwub"
      },
      "source": [
        "Verify the shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu5ZHPSAEwuc",
        "outputId": "20a406c6-1f93-496c-d296-89996c7047a0"
      },
      "source": [
        "offensive_train_text_data = offensive_train_text_data.drop(offensive_train_text_data.columns[1:], axis = 1)\n",
        "offensive_test_text_data = offensive_train_text_data.drop(offensive_train_text_data.columns[1:], axis = 1)\n",
        "offensive_val_text_data = offensive_train_text_data.drop(offensive_train_text_data.columns[1:], axis = 1)\n",
        "print(offensive_train_label_data.shape)\n",
        "print(offensive_train_text_data.shape)\n",
        "print(offensive_test_label_data.shape)\n",
        "print(offensive_test_text_data.shape)\n",
        "print(offensive_val_label_data.shape)\n",
        "print(offensive_val_text_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11916, 1)\n",
            "(11916, 1)\n",
            "(860, 1)\n",
            "(11916, 1)\n",
            "(1324, 1)\n",
            "(11916, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4YKhg_VEwuc"
      },
      "source": [
        "Combine text and label columns\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "J23sYFFOEwud",
        "outputId": "375b7cde-8c23-4f93-c1bb-f88f71aad69c"
      },
      "source": [
        "df_offensive_train = offensive_train_text_data.join(offensive_train_label_data,rsuffix = \"_2\")\n",
        "df_offensive_test = offensive_test_text_data.join(offensive_test_label_data,rsuffix = \"_2\")\n",
        "df_offensive_val = offensive_val_text_data.join(offensive_val_label_data,rsuffix = \"_2\")\n",
        "df_offensive_train.columns = ['text','label']\n",
        "df_offensive_test.columns = ['text','label']\n",
        "df_offensive_val.columns = ['text','label']\n",
        "df_offensive_val.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@user Bono... who cares. Soon people will unde...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@user Eight years the republicans denied obama...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@user Get him some line help. He is gonna be j...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@user @user She is great. Hi Fiona!</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@user She has become a parody unto herself? Sh...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  @user Bono... who cares. Soon people will unde...    0.0\n",
              "1  @user Eight years the republicans denied obama...    1.0\n",
              "2  @user Get him some line help. He is gonna be j...    0.0\n",
              "3                @user @user She is great. Hi Fiona!    0.0\n",
              "4  @user She has become a parody unto herself? Sh...    0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjI7euvuEwud"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knR7IQ1XEwue"
      },
      "source": [
        "Remove contradictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efigqMvzEwue"
      },
      "source": [
        " contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
        "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
        "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
        "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
        "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
        "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
        "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
        "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
        "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
        "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
        "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
        "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
        "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
        "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
        "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
        "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
        "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
        "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
        "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
        "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
        "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
        "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
        "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
        "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
        "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
        "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
        "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
        "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
        "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
        "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
        "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
        "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
        "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
        "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
        "                     \"you've\": \"you have\"}\n",
        "\n",
        "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "\n",
        "def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "  def replace(match):\n",
        "    return contractions_dict[match.group(0)]\n",
        "  return contractions_re.sub(replace, text)\n",
        "\n",
        "df_offensive_train['text']=df_offensive_train['text'].apply(lambda x:expand_contractions(x))\n",
        "df_offensive_test['text'] = df_offensive_test['text'].apply(lambda x:expand_contractions(x))\n",
        "df_offensive_val['text'] = df_offensive_val['text'].apply(lambda x:expand_contractions(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llFEZpztEwue"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3pYNJdMEwuf"
      },
      "source": [
        "df_offensive_train['text'] = df_offensive_train['text'].apply(processTweet)\n",
        "df_offensive_test['text'] = df_offensive_test['text'].apply(processTweet)\n",
        "df_offensive_val['text'] = df_offensive_val['text'].apply(processTweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QI_MM5rEwug"
      },
      "source": [
        "Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yb9vjPQtEwug"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Loading model\n",
        "nlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])\n",
        "\n",
        "df_offensive_train['text']=df_offensive_train['text'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))\n",
        "df_offensive_test['text']=df_offensive_test['text'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))\n",
        "df_offensive_val['text']=df_offensive_val['text'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZGsce-a3Ewug"
      },
      "source": [
        "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Yp4D8lFsEwuh"
      },
      "source": [
        "train_tokenized = df_offensive_train['text'].apply((lambda x: tokenizer.encode(x,add_special_tokens=True,padding=True,truncation=True)))\n",
        "test_tokenized = df_offensive_test['text'].apply((lambda x: tokenizer.encode(x,add_special_tokens=True,padding=True,truncation=True)))\n",
        "val_tokenized = df_offensive_val['text'].apply((lambda x: tokenizer.encode(x,add_special_tokens=True,padding=True,truncation=True)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4Xa1KQG4Ewuh"
      },
      "source": [
        "max_len = 0\n",
        "for i in train_tokenized.values:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "\n",
        "train_padded = np.array([i + [0]*(max_len-len(i)) for i in train_tokenized.values])\n",
        "test_padded = np.array([i + [0]*(max_len-len(i)) for i in test_tokenized.values])\n",
        "val_padded = np.array([i + [0]*(max_len-len(i)) for i in val_tokenized.values])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h70gMHhbEwuh",
        "outputId": "5b2a8c50-7dfc-41ff-eb38-075c4e07798e"
      },
      "source": [
        "print(np.array(test_padded).shape,np.array(train_padded).shape,np.array(val_padded).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11916, 65) (11916, 65) (11916, 65)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Lh6qtaZYEwuh",
        "outputId": "6765f147-4141-4300-94cc-b2d1e7ea356b"
      },
      "source": [
        "train_attention_mask = np.where(train_padded != 0, 1, 0)\n",
        "test_attention_mask = np.where(test_padded != 0, 1, 0)\n",
        "val_attention_mask = np.where(val_padded != 0,1,0)\n",
        "test_attention_mask.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11916, 65)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "OAFUkindEwui",
        "outputId": "069d08cf-a4a2-4657-8eda-84e90e724732"
      },
      "source": [
        "train_input_ids = torch.tensor(train_padded)  \n",
        "train_attention_mask = torch.tensor(train_attention_mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_last_hidden_states = model(train_input_ids, attention_mask=train_attention_mask)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5fc49e206029>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_last_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9XuVTdPEwuj"
      },
      "source": [
        "test_input_ids = torch.tensor(test_padded)\n",
        "test_attention_mask = torch.tensor(test_attention_mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "   test_last_hidden_states = model(test_input_ids, attention_mask=test_attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jMeW_nGEwuk"
      },
      "source": [
        "val_input_ids = torch.tensor(val_padded)\n",
        "val_attention_mask = torch.tensor(val_attention_mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "   val_last_hidden_states = model(val_input_ids, attention_mask=val_attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKNrtCbvEwuk"
      },
      "source": [
        "train_features = train_last_hidden_states[0][:,0,:].numpy()\n",
        "test_features = test_last_hidden_states[0][:,0,:].numpy()\n",
        "val_features = val_last_hidden_states[0][:,0,:].numpy()\n",
        "train_labels = torch.tensor(df_offensive_train['label'])\n",
        "test_labels = torch.tensor(df_offensive_test['label'])\n",
        "val_labels = torch.tensor(df_offensive_val['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gkbbjXIEwuk"
      },
      "source": [
        "random_forest.fit(train_features, train_labels)\n",
        "random_forest.score(test_features, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cypA1UTJEwuk"
      },
      "source": [
        "lr_clf = LogisticRegression(max_iter=1000)\n",
        "lr_clf.fit(train_features, train_labels)\n",
        "lr_clf.score(test_features, test_labels)\n",
        "lr_clf.score(val_features, val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRuQYtLrEwul"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "svc_regressor = SVC()\n",
        "\n",
        "scores = cross_val_score(svc_regressor, train_features, train_labels)\n",
        "print(\"SVM classifier average score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1Syuz7kEwul"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\n",
        "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\n",
        "grid.fit(train_features,train_labels)\n",
        "print(grid.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H6nihOtEwul"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix \n",
        "grid_predictions = grid.predict(test_features)\n",
        "print(confusion_matrix(test_labels,grid_predictions))\n",
        "print(classification_report(test_labels,grid_predictions))\n",
        "grid_predictions = grid.predict(val_features)\n",
        "print(classification_report(val_labels,grid_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK1wMFQpEwum"
      },
      "source": [
        "svc_regressor.fit(train_features, train_labels)\n",
        "svc_regressor.score(test_features, test_labels)\n",
        "print (\"accuracy from svm: \",svc_regressor.score(test_features, test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-boY82TiEwum"
      },
      "source": [
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "neural_network = MLPClassifier(hidden_layer_sizes=(2000), learning_rate='adaptive', verbose=False)\n",
        "neural_network.fit(train_features, train_labels)\n",
        "neural_network.score(test_features, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWQG283-Ewum"
      },
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "clf = DummyClassifier()\n",
        "\n",
        "scores = cross_val_score(clf, train_features, train_labels)\n",
        "print(\"Dummy classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKUgbjyEEwum"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "random_forest = RandomForestClassifier()\n",
        "scores = cross_val_score(random_forest, train_features, train_labels)\n",
        "print(\"Random forest classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFkct9GDEwun"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    if axes is None:\n",
        "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "    axes[0].set_title(title)\n",
        "    if ylim is not None:\n",
        "        axes[0].set_ylim(*ylim)\n",
        "    axes[0].set_xlabel(\"Training examples\")\n",
        "    axes[0].set_ylabel(\"Score\")\n",
        "\n",
        "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
        "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
        "                       train_sizes=train_sizes,\n",
        "                       return_times=True)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    fit_times_mean = np.mean(fit_times, axis=1)\n",
        "    fit_times_std = np.std(fit_times, axis=1)\n",
        "\n",
        "    # Plot learning curve\n",
        "    axes[0].grid()\n",
        "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                         color=\"r\")\n",
        "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                         color=\"g\")\n",
        "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "                 label=\"Training score\")\n",
        "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "                 label=\"Cross-validation score\")\n",
        "    axes[0].legend(loc=\"best\")\n",
        "\n",
        "    # Plot n_samples vs fit_times\n",
        "    axes[1].grid()\n",
        "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
        "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
        "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
        "    axes[1].set_xlabel(\"Training examples\")\n",
        "    axes[1].set_ylabel(\"fit_times\")\n",
        "    axes[1].set_title(\"Scalability of the model\")\n",
        "\n",
        "    # Plot fit_time vs score\n",
        "    axes[2].grid()\n",
        "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
        "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
        "    axes[2].set_xlabel(\"fit_times\")\n",
        "    axes[2].set_ylabel(\"Score\")\n",
        "    axes[2].set_title(\"Performance of the model\")\n",
        "\n",
        "    return plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fLudUnYEwun"
      },
      "source": [
        "title = r\"Learning Curves (SVM, sigmoid kernel, $\\gamma=0.001$)\"\n",
        "# SVC is more expensive so we do a lower number of CV iterations:\n",
        "cv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=15)\n",
        "estimator = SVC(gamma=0.001,kernel = 'sigmoid')\n",
        "plot_learning_curve(estimator, title,train_features, train_labels,cv=cv, n_jobs=4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyqVror7E7AB"
      },
      "source": [
        "datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u3tVUB9E7AH"
      },
      "source": [
        "sentiment_train_label_data =  pd.read_fwf('https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/train_labels.txt',header=None)\n",
        "sentiment_train_text_data = pd.read_fwf('https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/train_text.txt',header=None)\n",
        "sentiment_test_label_data =pd.read_fwf ('https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/test_labels.txt',header=None)\n",
        "sentiment_test_text_data = pd.read_fwf ('https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/test_text.txt',header=None)\n",
        "sentiment_val_label_data = pd.read_fwf ('https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/val_labels.txt',header=None)\n",
        "sentiment_val_text_data = pd.read_fwf ('https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/val_text.txt',header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIItdq3sE7AI"
      },
      "source": [
        "Verify the shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "id": "WW1fh0CbE7AK",
        "outputId": "c9fc0a2f-c3bd-4b76-801a-de32af37c0e8"
      },
      "source": [
        "sentiment_train_text_data = sentiment_train_text_data.drop(sentiment_train_text_data.columns[1], axis = 1)\n",
        "sentiment_val_text_data = sentiment_val_text_data.drop(sentiment_val_text_data.columns[1], axis = 1)\n",
        "print(sentiment_train_label_data.shape)\n",
        "print(sentiment_train_text_data.shape)\n",
        "print(sentiment_test_label_data.shape)\n",
        "print(sentiment_test_text_data.shape)\n",
        "print(sentiment_val_label_data.shape)\n",
        "print(sentiment_val_text_data.shape)\n",
        "sentiment_val_text_data.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45615, 1)\n",
            "(45615, 1)\n",
            "(12284, 1)\n",
            "(12284, 1)\n",
            "(2000, 1)\n",
            "(2000, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dark Souls 3 April Launch Date Confirmed With ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"National hot dog day, national tequila day, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>When girls become bandwagon fans of the Packer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@user I may or may not have searched it up on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Here's your starting TUESDAY MORNING Line up a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>@user F-Main, are you in the office tomorrow i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>#US 1st Lady Michelle Obama speaking at the 20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Omg this show is so predictable even for the 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>\"What a round by Paul Dunne, good luck tomorro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Irving Plaza NYC Blackout Saturday night. Got ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>@user so the thing next Thursday isn't free, y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Can't wait for tomorrow at 9 pm! Chelsea v cry...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>We just received more tickets for Blue Rodeo a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>RT @user Republicans thought they were smart t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>F*** the hurricane party this Tues santosparty...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>\"Thank you @user for the message. I'm very pro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Tom Brady is locked for Thursday. Let the seas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Why do y'all want Nicki to be pregnant so bad ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Eric Church is headlining a festival in Raleig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>The sad part about this is tomorrow Nicki will...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    0\n",
              "0   Dark Souls 3 April Launch Date Confirmed With ...\n",
              "1   \"National hot dog day, national tequila day, t...\n",
              "2   When girls become bandwagon fans of the Packer...\n",
              "3   @user I may or may not have searched it up on ...\n",
              "4   Here's your starting TUESDAY MORNING Line up a...\n",
              "5   @user F-Main, are you in the office tomorrow i...\n",
              "6   #US 1st Lady Michelle Obama speaking at the 20...\n",
              "7   Omg this show is so predictable even for the 3...\n",
              "8   \"What a round by Paul Dunne, good luck tomorro...\n",
              "9   Irving Plaza NYC Blackout Saturday night. Got ...\n",
              "10  @user so the thing next Thursday isn't free, y...\n",
              "11  Can't wait for tomorrow at 9 pm! Chelsea v cry...\n",
              "12  We just received more tickets for Blue Rodeo a...\n",
              "13  RT @user Republicans thought they were smart t...\n",
              "14  F*** the hurricane party this Tues santosparty...\n",
              "15  \"Thank you @user for the message. I'm very pro...\n",
              "16  Tom Brady is locked for Thursday. Let the seas...\n",
              "17  Why do y'all want Nicki to be pregnant so bad ...\n",
              "18  Eric Church is headlining a festival in Raleig...\n",
              "19  The sad part about this is tomorrow Nicki will..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfUwoKscE7AM"
      },
      "source": [
        "Combine text and label columns\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "56oMeY4dE7AN",
        "outputId": "a7fdeccf-d670-4149-cd76-529ffe958697"
      },
      "source": [
        "df_sentiment_train = sentiment_train_text_data.join(sentiment_train_label_data,rsuffix = \"_2\")\n",
        "df_sentiment_test = sentiment_test_text_data.join(sentiment_test_label_data,rsuffix = \"_2\")\n",
        "df_sentiment_val = sentiment_val_text_data.join(sentiment_val_label_data,rsuffix = \"_2\")\n",
        "df_sentiment_train.columns = ['text','label']\n",
        "df_sentiment_test.columns = ['text','label']\n",
        "df_sentiment_val.columns = ['text','label']\n",
        "df_sentiment_val.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dark Souls 3 April Launch Date Confirmed With ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"National hot dog day, national tequila day, t...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>When girls become bandwagon fans of the Packer...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@user I may or may not have searched it up on ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Here's your starting TUESDAY MORNING Line up a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  Dark Souls 3 April Launch Date Confirmed With ...      1\n",
              "1  \"National hot dog day, national tequila day, t...      2\n",
              "2  When girls become bandwagon fans of the Packer...      0\n",
              "3  @user I may or may not have searched it up on ...      1\n",
              "4  Here's your starting TUESDAY MORNING Line up a...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHEOxIodE7AO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzrJz511E7AP"
      },
      "source": [
        "Remove contradictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRJra88hE7AP"
      },
      "source": [
        " contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
        "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
        "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
        "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
        "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
        "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
        "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
        "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
        "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
        "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
        "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
        "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
        "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
        "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
        "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
        "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
        "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
        "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
        "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
        "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
        "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
        "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
        "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
        "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
        "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
        "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
        "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
        "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
        "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
        "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
        "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
        "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
        "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
        "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
        "                     \"you've\": \"you have\"}\n",
        "\n",
        "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "\n",
        "def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "  def replace(match):\n",
        "    return contractions_dict[match.group(0)]\n",
        "  return contractions_re.sub(replace, text)\n",
        "\n",
        "df_sentiment_train['text']=df_sentiment_train['text'].apply(lambda x:expand_contractions(x))\n",
        "df_sentiment_test['text'] = df_sentiment_test['text'].apply(lambda x:expand_contractions(x))\n",
        "df_sentiment_val['text'] = df_sentiment_val['text'].apply(lambda x:expand_contractions(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT1-_xYoE7AP"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFWndtRSE7AQ"
      },
      "source": [
        "df_sentiment_train['text'] = df_sentiment_train['text'].apply(processTweet)\n",
        "df_sentiment_test['text'] = df_sentiment_test['text'].apply(processTweet)\n",
        "df_sentiment_val['text'] = df_sentiment_val['text'].apply(processTweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBv2Fv-jE7AQ"
      },
      "source": [
        "Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hutJUMr1E7AR"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Loading model\n",
        "nlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])\n",
        "\n",
        "df_sentiment_train['text']=df_sentiment_train['text'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))\n",
        "df_sentiment_test['text']=df_sentiment_test['text'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))\n",
        "df_sentiment_val['text']=df_sentiment_val['text'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265,
          "referenced_widgets": [
            "d753a775b53040d0afd2cd3d1c081427",
            "1ca8e789ed764b6a935aaaadce8cff60",
            "64df059ce1ba4a079f1bb31df56a4fd7",
            "0a13acabb9d740359ffaadea4c4fdcaf",
            "c687225926af40b7b73ac26c60b72f83",
            "4952d720526440f097f7efb823970181",
            "e17898d5a60d4bf48b8cb7bd5901cf91",
            "fb4c6fc1e9514e9da444b883d91f72f9",
            "04ed2c368a3847eab652c7a7215305be",
            "b4b6cafe3b274e54a4eb925d2510c97e",
            "ffcd0a796a92436db4c3fac3616317c1",
            "69f727f4114946d099f95d385accc8d4",
            "98410f5a3c6e4318aaa95bb50b8b8331",
            "3e1c89ad6ed84aa88abbd9bf5a0ad31a",
            "61fd82f873f949fa87d16ebc023e4226",
            "2fc0f9839d0f41b68466154d475b9867",
            "c65c6a713b3949bbab9521417cd28b27",
            "6b5c528c5eb447bb8424aa6dc52412ce",
            "6dadb3d934694d399fefd8bf0c554dbe",
            "06aa7037100945fdbfc1d2dbc957497c",
            "f425b2578d8d497187b5cfed06513a4c",
            "ad1bb9ec3a0e41278e4dd468d20bc482",
            "a4c36b1e8cf74e068cb170a9e29b35ed",
            "d13fda84a6404cfa9ca761e7e216baf9",
            "c46ad39896f446f9bacd7838239a5f71",
            "9868c964b7d843f6853c64d6978f1a07",
            "7e228fc0f00f4b9581df026dbfd9fe84",
            "f8588841440c4e8699fcfbe9a47c927e",
            "2e1ffa7253bb426abcd52f2c4472cb11",
            "818cb048cab84fa699ddb9277785d379",
            "8d9bf83235334f31aff11066593a9eae",
            "5f2898082ff040da9e64c8b3d9fdfb25",
            "70caa05cc5f04d84ba9c0bd674048961",
            "aa91ad5acbc14e5c83cd85cfb5fe7473",
            "c158323df6a44526bb967ba67f12dcdc",
            "309104cdb3ba4d65a5684d9e1b93beb2",
            "30d7206f41164d18819303bb2a5a4dcf",
            "8d092cda85f64599806c092141cbf95c",
            "a5417c3f99a54023a43d5f44f25b82c4",
            "5c633c6215b04b86817a362f4b01fee8"
          ]
        },
        "id": "Bxn8KfjHE7AR",
        "outputId": "df840481-f763-4407-ae56-ef46651e857a"
      },
      "source": [
        "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d753a775b53040d0afd2cd3d1c081427",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04ed2c368a3847eab652c7a7215305be",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c65c6a713b3949bbab9521417cd28b27",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c46ad39896f446f9bacd7838239a5f71",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70caa05cc5f04d84ba9c0bd674048961",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzgSUlcTE7AR"
      },
      "source": [
        "train_tokenized = df_sentiment_train['text'].apply((lambda x: tokenizer.encode(x,add_special_tokens=True,padding=True,truncation=True)))\n",
        "test_tokenized = df_sentiment_test['text'].apply((lambda x: tokenizer.encode(x,add_special_tokens=True,padding=True,truncation=True)))\n",
        "val_tokenized = df_sentiment_val['text'].apply((lambda x: tokenizer.encode(x,add_special_tokens=True,padding=True,truncation=True)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AblVB02rE7AS"
      },
      "source": [
        "max_len = 0\n",
        "for i in train_tokenized.values:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "\n",
        "train_padded = np.array([i + [0]*(70-len(i)) for i in train_tokenized.values])\n",
        "test_padded = np.array([i + [0]*(70-len(i)) for i in test_tokenized.values])\n",
        "val_padded = np.array([i + [0]*(70-len(i)) for i in val_tokenized.values])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrFksRedE7AS",
        "outputId": "0646c6ed-e6c2-40d5-ce46-19c759025bc0"
      },
      "source": [
        "print(np.array(test_padded).shape,np.array(train_padded).shape,np.array(val_padded).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12284, 70) (45615, 70) (2000, 70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL2Knv3WE7AS",
        "outputId": "fc9ec53a-8268-4d09-940d-7505d883e67d"
      },
      "source": [
        "train_attention_mask = np.where(train_padded != 0, 1, 0)\n",
        "test_attention_mask = np.where(test_padded != 0, 1, 0)\n",
        "val_attention_mask = np.where(val_padded != 0,1,0)\n",
        "test_attention_mask.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12284, 70)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBNkRwGrE7AT"
      },
      "source": [
        "train_input_ids = torch.tensor(train_padded)  \n",
        "train_attention_mask = torch.tensor(train_attention_mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_last_hidden_states = model(train_input_ids, attention_mask=train_attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YLvlWpiE7AT"
      },
      "source": [
        "test_input_ids = torch.tensor(test_padded)\n",
        "test_attention_mask = torch.tensor(test_attention_mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "   test_last_hidden_states = model(test_input_ids, attention_mask=test_attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw8-o6ibE7AT"
      },
      "source": [
        "val_input_ids = torch.tensor(val_padded)\n",
        "val_attention_mask = torch.tensor(val_attention_mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "   val_last_hidden_states = model(val_input_ids, attention_mask=val_attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVBJ34EAE7AT"
      },
      "source": [
        "train_features = train_last_hidden_states[0][:,0,:].numpy()\n",
        "test_features = test_last_hidden_states[0][:,0,:].numpy()\n",
        "val_features = val_last_hidden_states[0][:,0,:].numpy()\n",
        "train_labels = torch.tensor(df_sentiment_train['label'])\n",
        "test_labels = torch.tensor(df_sentiment_test['label'])\n",
        "val_labels = torch.tensor(df_sentiment_val['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4LPUnChE7AT"
      },
      "source": [
        "random_forest.fit(train_features, train_labels)\n",
        "random_forest.score(test_features, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZdxRqP3E7AU"
      },
      "source": [
        "lr_clf = LogisticRegression(max_iter=1000)\n",
        "lr_clf.fit(train_features, train_labels)\n",
        "lr_clf.score(test_features, test_labels)\n",
        "lr_clf.score(val_features, val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENUbkHIiE7AU"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "svc_regressor = SVC()\n",
        "\n",
        "scores = cross_val_score(svc_regressor, train_features, train_labels)\n",
        "print(\"SVM classifier average score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBoutI3mE7AU"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\n",
        "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\n",
        "grid.fit(train_features,train_labels)\n",
        "print(grid.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJJLN9luE7AU"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix \n",
        "grid_predictions = grid.predict(test_features)\n",
        "print(confusion_matrix(test_labels,grid_predictions))\n",
        "print(classification_report(test_labels,grid_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSFheO5iE7AU"
      },
      "source": [
        "svc_regressor.fit(train_features, train_labels)\n",
        "svc_regressor.score(test_features, test_labels)\n",
        "print (\"accuracy from svm: \",svc_regressor.score(val_features, val_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJTMCuI3E7AV"
      },
      "source": [
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "neural_network = MLPClassifier(hidden_layer_sizes=(2000), learning_rate='adaptive', verbose=False)\n",
        "neural_network.fit(train_features, train_labels)\n",
        "neural_network.score(test_features, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aTqnEKKE7AV"
      },
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "clf = DummyClassifier()\n",
        "\n",
        "scores = cross_val_score(clf, train_features, train_labels)\n",
        "print(\"Dummy classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEWOqYr-E7AV"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "random_forest = RandomForestClassifier()\n",
        "scores = cross_val_score(random_forest, train_features, train_labels)\n",
        "print(\"Random forest classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0qgMWKpE7AV"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    if axes is None:\n",
        "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "    axes[0].set_title(title)\n",
        "    if ylim is not None:\n",
        "        axes[0].set_ylim(*ylim)\n",
        "    axes[0].set_xlabel(\"Training examples\")\n",
        "    axes[0].set_ylabel(\"Score\")\n",
        "\n",
        "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
        "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
        "                       train_sizes=train_sizes,\n",
        "                       return_times=True)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    fit_times_mean = np.mean(fit_times, axis=1)\n",
        "    fit_times_std = np.std(fit_times, axis=1)\n",
        "\n",
        "    # Plot learning curve\n",
        "    axes[0].grid()\n",
        "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                         color=\"r\")\n",
        "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                         color=\"g\")\n",
        "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "                 label=\"Training score\")\n",
        "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "                 label=\"Cross-validation score\")\n",
        "    axes[0].legend(loc=\"best\")\n",
        "\n",
        "    # Plot n_samples vs fit_times\n",
        "    axes[1].grid()\n",
        "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
        "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
        "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
        "    axes[1].set_xlabel(\"Training examples\")\n",
        "    axes[1].set_ylabel(\"fit_times\")\n",
        "    axes[1].set_title(\"Scalability of the model\")\n",
        "\n",
        "    # Plot fit_time vs score\n",
        "    axes[2].grid()\n",
        "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
        "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
        "    axes[2].set_xlabel(\"fit_times\")\n",
        "    axes[2].set_ylabel(\"Score\")\n",
        "    axes[2].set_title(\"Performance of the model\")\n",
        "\n",
        "    return plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChF78PAdE7AV"
      },
      "source": [
        "title = r\"Learning Curves (SVM, sigmoid kernel, $\\gamma=0.001$)\"\n",
        "# SVC is more expensive so we do a lower number of CV iterations:\n",
        "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
        "estimator = SVC(gamma=0.001,kernel = 'sigmoid')\n",
        "plot_learning_curve(estimator, title,train_features, train_labels,cv=cv, n_jobs=4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}